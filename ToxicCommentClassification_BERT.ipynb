{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicCommentClassification_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxFagI9XQRbU"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbFoKO3aNwEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7191c2-3fc0-4277-c1dc-d7e954afcd85"
      },
      "source": [
        "# import necessary files\n",
        "import pandas as pd\n",
        "# import seaborn as sns\n",
        "import numpy as np\n",
        "# import spacy\n",
        "# import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-fe_l7ENKkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9d0b63-329e-4f49-92c8-b2feb81ca682"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUfyAtI4FZND"
      },
      "source": [
        "# data cleaning method: removes http links, special characters and user tagged comments \n",
        "def clean_data(text):\n",
        "  text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
        "  text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
        "  text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
        "  text = text.map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
        "  return text\n",
        "\n",
        "stopset = set(stopwords.words('english'))\n",
        "stopset.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
        "\n",
        "# Method to remove stopwords\n",
        "def remove_stopword(text):\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  processed = []\n",
        "  for comment in tqdm(text):\n",
        "    tokens = tokenizer.tokenize(comment)\n",
        "    word_list = [word for word in tokens if word not in stopset]\n",
        "    processed.append(\" \".join(word_list))\n",
        "  return processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzClhUuvdKYh"
      },
      "source": [
        "To run this notebook, place the data folder into your Google Drive and change the data path below (line 2) to your data directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW2lYlARNTLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219aee9b-8fc6-4492-8bf0-1ec8791b947c"
      },
      "source": [
        "# read train and test data\n",
        "data_path = \"/content/drive/My Drive/NLP_Data/\"\n",
        "train = pd.read_csv(data_path+\"train.csv\")\n",
        "train = train.fillna('na')\n",
        "test = pd.read_csv(data_path+\"test.csv\")\n",
        "test = test.fillna('na')\n",
        "test_labels = pd.read_csv(data_path+\"test_labels.csv\")\n",
        "# print(train.columns)\n",
        "\n",
        "#remove rows from test set that have -1s\n",
        "test_with_labels = pd.concat([test, test_labels], axis = 1)\n",
        "test_with_labels = test_with_labels[test_with_labels['toxic'] != -1]\n",
        "\n",
        "#remove unnecessary columns\n",
        "X_train = train['comment_text']\n",
        "X_test = test_with_labels['comment_text']\n",
        "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']]\n",
        "y_test = test_with_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']]\n",
        "\n",
        "# Cleaning the df \n",
        "clean_train = clean_data(X_train).tolist()\n",
        "clean_test = clean_data(X_test).tolist()\n",
        "\n",
        "# Removing stop words \n",
        "clean_train = remove_stopword(clean_train)\n",
        "clean_test = remove_stopword(clean_test)\n",
        "\n",
        "X_train1 = pd.DataFrame(clean_train, columns=['comment_text'])\n",
        "X_test1 = pd.DataFrame(clean_test, columns=['comment_text'])\n",
        "\n",
        "# creating a subset of each dataset in order to run relatively fast with BERT\n",
        "X_train_subset = X_train1[:2000]\n",
        "X_test_subset = X_test1[:400]\n",
        "y_train_subset = y_train[:2000]\n",
        "y_test_subset = y_test[:400]\n",
        "\n",
        "# Splitting training data into train and validation sets \n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_subset, y_train_subset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Printing lengths of train, test and validation to split after vectorization \n",
        "print(\"length of train is ...\" + str(len(X_train)))\n",
        "print(\"length of validation is ...\" + str(len(X_val)))\n",
        "print(\"length of test is ...\" + str(len(X_test_subset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total time 4.5119030475616455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 3940/159571 [00:00<00:03, 39396.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total time 1.79317307472229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 159571/159571 [00:04<00:00, 37094.60it/s]\n",
            "100%|██████████| 63978/63978 [00:01<00:00, 39025.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "length of train is ...1600\n",
            "length of validation is ...400\n",
            "length of test is ...400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvBNTgzANAXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c997d8a-d4c5-4f09-b00c-11a2dc347f9b"
      },
      "source": [
        "print(X_train_subset.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                        comment_text\n",
            "0  Explanation Why edits made username Hardcore M...\n",
            "1  D aww He matches background colour I seemingly...\n",
            "2  Hey man I really trying edit war It guy consta...\n",
            "3  More I make real suggestions improvement I won...\n",
            "4              You sir hero Any chance remember page\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k0XHS2OZVXM"
      },
      "source": [
        "# Function for vectorization using BERT\n",
        "def bert_vectorize(df, model):\n",
        "    start_time=time.time()\n",
        "    sentences = df['comment_text'].tolist()\n",
        "    #load pretrained BERT model\n",
        "    #encode sentences\n",
        "    vectors = model.encode(sentences)\n",
        "    end_time=time.time()\n",
        "    print(\"total time to vectorize: \",end_time-start_time)\n",
        "    return list(vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw-2OnT5vmWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b419e792-5e3a-474e-a9c6-b07c992892ed"
      },
      "source": [
        "# Vectorize data using BERT\n",
        "model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "vectorized_X_train_subset = bert_vectorize(X_train, model)\n",
        "vectorized_X_val = bert_vectorize(X_val, model)\n",
        "vectorized_X_test = bert_vectorize(X_test_subset, model)\n",
        "\n",
        "# verifying lengths of train, test and validation after vectorization \n",
        "print(\"length of train is ...\" + str(len(vectorized_X_train_subset)))\n",
        "print(\"length of validation is ...\" + str(len(vectorized_X_val)))\n",
        "print(\"length of test is ...\" + str(len(X_test_subset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:18<00:00, 21.7MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total time to vectorize:  251.22597742080688\n",
            "total time to vectorize:  64.99932622909546\n",
            "total time to vectorize:  85.3821349143982\n",
            "length of train is ...1600\n",
            "length of validation is ...400\n",
            "length of test is ...400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWwz2QGSMFOu"
      },
      "source": [
        "# classify data: LogReg\n",
        "def logreg(X_train, y_train, X_test, y_test):\n",
        "  text_classifier = LogisticRegression(solver='sag', C=10)\n",
        "  lr_pipeline = Pipeline([('clf', OneVsRestClassifier(text_classifier)),])\n",
        "  categories = list(y_train.columns.values)\n",
        "  print(categories)\n",
        "  y_pred = []\n",
        "  for category in categories:\n",
        "      print(category)\n",
        "      \n",
        "      # Training logistic regression model on train data\n",
        "      lr_pipeline.fit(X_train, y_train[category])\n",
        "      \n",
        "      # calculating test accuracy\n",
        "      predictions = lr_pipeline.predict(X_test)\n",
        "      y_pred.append(predictions)\n",
        "      print('Test accuracy is {}'.format(accuracy_score(y_true=y_test[category], y_pred=predictions)))\n",
        "      print(\"\\n\")\n",
        "\n",
        "      print('Test F1 score is {}'.format(f1_score(y_true=y_test[category], y_pred=predictions, average='weighted')))\n",
        "      print('\\n')\n",
        "\n",
        "      print('classification report per label')\n",
        "      print(classification_report(y_pred=predictions, y_true=y_test[category]))\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vFlJ1HU9GBZ"
      },
      "source": [
        "# classify validation data \n",
        "val_pred = logreg(vectorized_X_train_subset, y_train, vectorized_X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq9dzoUdLVeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a38b4d-85f9-4adf-81aa-b655e81d871c"
      },
      "source": [
        "# classify test data\n",
        "# this time don't split X_train into train and val sets\n",
        "vectorized_X_train = bert_vectorize(X_train_subset, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total time to vectorize:  311.8511230945587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5UzDxrIB14X"
      },
      "source": [
        "y_pred = logreg(vectorized_X_train, y_train_subset, vectorized_X_test, y_test_subset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU3OeEcG8VBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69993bef-1bd2-48de-bf73-77ff614375ab"
      },
      "source": [
        "# convert y_pred to df with each category's predictions getting slotted into the corresponding column\n",
        "y_pred_df = pd.DataFrame(columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
        "y_pred_df[\"toxic\"] = y_pred[0]\n",
        "y_pred_df[\"severe_toxic\"] = y_pred[1]\n",
        "y_pred_df[\"obscene\"] = y_pred[2]\n",
        "y_pred_df[\"threat\"] = y_pred[3]\n",
        "y_pred_df[\"insult\"] = y_pred[4]\n",
        "y_pred_df[\"identity_hate\"] = y_pred[5]\n",
        "print(y_pred_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
            "0        0             0        0       0       0              0\n",
            "1        1             0        0       0       0              0\n",
            "2        0             0        0       0       0              0\n",
            "3        0             0        0       0       0              0\n",
            "4        0             0        0       0       0              0\n",
            "..     ...           ...      ...     ...     ...            ...\n",
            "395      0             0        0       0       0              0\n",
            "396      0             0        0       0       0              0\n",
            "397      0             0        0       0       0              0\n",
            "398      0             0        0       0       0              0\n",
            "399      0             0        0       0       0              0\n",
            "\n",
            "[400 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pi8YJ7tQ0Z3"
      },
      "source": [
        "print(classification_report(y_pred=y_pred_df['toxic'], y_true=y_test_subset['toxic']))\n",
        "print(classification_report(y_pred=y_pred_df['severe_toxic'], y_true=y_test_subset['severe_toxic']))\n",
        "print(classification_report(y_pred=y_pred_df['obscene'], y_true=y_test_subset['obscene']))\n",
        "print(classification_report(y_pred=y_pred_df['threat'], y_true=y_test_subset['threat']))\n",
        "print(classification_report(y_pred=y_pred_df['insult'], y_true=y_test_subset['insult']))\n",
        "print(classification_report(y_pred=y_pred_df['identity_hate'], y_true=y_test_subset['identity_hate']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xJtOrux-3cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3787f6-75bd-4acc-b6e6-88a836bb2b46"
      },
      "source": [
        "print(y_test_subset)\n",
        "print(roc_auc_score(y_true=y_test_subset[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values, y_score=y_pred_df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
            "5        0             0        0       0       0              0\n",
            "7        0             0        0       0       0              0\n",
            "11       0             0        0       0       0              0\n",
            "13       0             0        0       0       0              0\n",
            "14       0             0        0       0       0              0\n",
            "..     ...           ...      ...     ...     ...            ...\n",
            "931      0             0        0       0       0              0\n",
            "934      0             0        0       0       0              0\n",
            "935      0             0        0       0       0              0\n",
            "937      0             0        0       0       0              0\n",
            "939      0             0        0       0       0              0\n",
            "\n",
            "[400 rows x 6 columns]\n",
            "0.6107227151130059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnqnqZHp_Rsq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}